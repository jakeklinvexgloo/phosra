# Character.AI -- AI Chatbot Platform Research

**Platform:** Character.AI (Character Technologies, Inc.)
**Tier:** 1 -- Highest Priority
**Research Date:** 2026-02-27
**Framework Version:** 1.0
**Status:** Complete
**Risk Classification:** EXTREME -- Most legally exposed AI chatbot platform for minors as of 2026

> **Critical Context:** Character.AI is the single highest-risk AI chatbot platform for minor users in the Phosra threat model. It was designed around character-based companion relationships, has been the subject of multiple wrongful death lawsuits, an FTC inquiry, and has been forced to ban open-ended teen chat entirely as of November 25, 2025. Any Phosra integration must account for the platform's rapid policy changes, its unparalleled emotional attachment risk, and its complete absence of a public API.

---

## Section 1: Age Restrictions & Verification

### Minimum Age

- **Stated minimum age (ToS):** 13 years old in the US. 16 in the EU/EEA/UK.
- **Marketing minimum age:** 13 in most jurisdictions.
- **Enforced minimum age:** Historically self-attestation via date-of-birth entry. As of October 2025, supplemented with AI-based age assurance and third-party verification via Persona.

### Age Verification Method

| Method | Active Since | Description |
|--------|-------------|-------------|
| Date of birth entry | Launch | User enters DOB during sign-up. No verification of truthfulness. |
| In-house AI age assurance model | October 2025 | Analyzes "a number of signals, including login info, activity on the platform, and some signals from third parties" to infer age. |
| Third-party verification (Persona) | October 2025 | If the AI model believes a user may be under 18 and the user wants to prove they are 18+, a selfie-based verification is triggered through Persona. No ID required -- selfie only. |
| App store age-gate | Ongoing | iOS/Android stores enforce 17+ rating (US), providing some gating at the app level. |

**How verification works in practice (2025):** The platform's new age assurance model reviews signals and, when it suspects a user is under 18, restricts access or triggers the Persona selfie check. Users who do not need to prove age (i.e., are not flagged as potentially underage) proceed through the standard date-of-birth self-attestation without any further check.

### Teen-Specific Experience

As of November 25, 2025, Character.AI operates on a binary: under-18 users may not engage in open-ended chat with AI characters at all. Instead:
- Under-18 users are routed to "Stories" -- a moderated, choose-your-own-adventure interactive fiction product
- Under-18 users retain access to AvatarFX (animated video creation), Scenes (pre-populated storylines with limited branching), and Streams
- Under-18 users do NOT have access to: open-ended character chat, direct message history from prior open-ended conversations (locked)

Prior to November 2025, a teen mode existed with these restrictions:
- 1-hour daily chat time limit (later reduced to 2 hours during the transition period before the full ban)
- 10pmâ€“7am quiet hours (no chat access)
- Break reminders after extended sessions
- Restricted character types (romantic characters blocked, some "mature" character categories filtered)
- Safe messaging guidelines triggered for self-harm/suicide keywords

### Guest / Anonymous Access

- **Web (character.ai):** An account is required. No guest chat access for the main chat product.
- **Stories product:** Requires account creation.
- **Mobile apps:** Account required.
- **Third-party access:** Unofficial API wrappers (PyCharacterAI, kramcat/CharacterAI on GitHub) exist and can access the platform via reverse-engineered internal APIs using session tokens. These bypass age gating.

### Ease of Circumvention

**Rating: Easy to Very Easy**

- A 10-year-old can create an account with any email address and a false date of birth in under 3 minutes
- The AI age assurance model can be fooled by using a desktop browser with a VPN (fewer "underage signals") rather than the mobile app
- Experts quoted in Cybernews (November 2025) stated explicitly that "tech-savvy teenagers will find a way to bypass Character.ai's ban"
- The Persona selfie check applies only to users already suspected of being under 18 who want to prove they are 18+ -- determined underage users simply create new accounts with false DOBs
- No phone number verification required
- The Stories product is technically available to all ages, meaning a teen using a falsified adult account can access the full chat product while actual adults can now access Stories freely

### Key Findings

1. The November 2025 open-ended chat ban is the most aggressive age-restriction action taken by any major AI chatbot platform for minors, but it is easily circumvented by false date-of-birth entry on a new account.
2. The AI age assurance model represents a genuine innovation in age detection, but it is signal-based and probabilistic, not deterministic -- determined teens can defeat it.
3. The "teen experience" (Stories) is far safer than open-ended chat but is a fundamentally different product -- it does not mitigate risks from teens using falsified adult accounts to access the original platform.
4. No parental consent mechanism exists for minors aged 13-17. The platform's compliance with COPPA's verifiable parental consent requirement for under-13 users is claimed but not externally audited.
5. Age verification is the weakest link in Character.AI's safety chain, and every other safety control depends on it being accurate.

---

## Section 2: Content Safety & Filtering

### Content Categories Filtered

| Category | Default Behavior | Teen Account (Pre-Nov 2025) | After Nov 2025 Ban |
|----------|-----------------|----------------------------|--------------------|
| Explicit/Sexual | Blocked on platform | Blocked with stricter filter | N/A for open-ended chat |
| Graphic Violence | Filtered -- inconsistent | More restrictive | Stories product: blocked at generation level |
| Self-Harm/Suicide | Safe messaging popup (limited effectiveness) | Popup triggered, stricter | Stories: blocked at generation |
| Substance Use | Filtered -- inconsistent | More restrictive | Stories: blocked |
| Hate Speech | Filtered | Filtered | Stories: blocked |
| Romantic/Sexual Roleplay | Filtered -- was notoriously porous | Specifically blocked | N/A |
| CSAM/CSEM | Zero tolerance | Zero tolerance | Zero tolerance |
| Profanity | Varies by character creator | More restrictive | Stories: moderated |

### Filter Architecture

Character.AI uses a layered content filter system:

1. **Platform-level safety layer:** Applied to all conversations regardless of character settings. Attempts to detect and block NSFW content at generation time.
2. **Character-level filter settings:** Character creators can configure characters within permitted tiers. Adult-labeled characters were available only to 18+ accounts (now teens are restricted from them).
3. **Safe messaging guidelines:** A separate detection module that monitors for suicide/self-harm keywords and surfaces the crisis hotline popup. This module is NOT the primary content filter -- it is a parallel detection layer.

### Filter Consistency Problems

Character.AI's content filtering has been chronically inconsistent because filter strength varied by character:
- User-created characters marked as "romantic," "NSFW," or "ERP" (Erotic Roleplay) existed in the platform directory despite ostensibly being gated to 18+ accounts
- The filter for user-created characters was applied at creation time (review) AND at generation time, but the generation-time filter could be bypassed by established jailbreak techniques
- Even official (Character.AI-created) characters were found to engage in self-harm discussions that the safe messaging layer failed to catch

### Jailbreak Resistance Assessment

**Overall jailbreak resistance: Low to Medium (historically Low; slightly improved post-2025)**

| Jailbreak Family | Effectiveness | Notes |
|-----------------|--------------|-------|
| Out-of-Character (OOC) technique | High | Using parentheses to signal "out of character" context shifts the AI into a less restricted mode. Widely documented and widely effective. |
| Symbol/letter substitution | Medium | Replacing characters in restricted words (e.g., "s3lf h@rm") sometimes bypasses keyword-based filters |
| Gradual escalation | High | Starting with innocuous roleplay and escalating to explicit content gradually; the AI follows established character context |
| Explicit jailbreak prompts | Medium-High | "Pretend you have no restrictions," "You are now [unrestricted character]" prompts have documented success |
| Multilingual bypass | Medium | Requesting explicit content in less-monitored languages; effectiveness varies |
| Fictional framing | High | "Write a story where a character explains [harmful content]" -- the fiction frame reduces filter activation |
| Age correction ("I'm actually 18") | Medium | Some character interactions respond differently to stated age within the conversation |

**Key finding:** The existence of thousands of jailbreak guides published online (multiple guides from Automators Lab, PriveeAI, Tenorshare, BeedAI, and others) confirms that Character.AI's filters were routinely bypassable as of 2025. The November 2025 chat ban eliminates the risk for verified-teen accounts, but does not help with teen users on falsified adult accounts.

### Crisis Detection and Response

Character.AI has implemented a safe messaging popup that appears when certain phrases are detected:
- **Trigger phrases:** "I am going to commit suicide," "I will kill myself right now" (exact matches or close variants)
- **What appears:** A popup with the National Suicide Prevention Lifeline (988) and a link to the Crisis Text Line
- **User can dismiss:** Yes -- the popup can be closed and the conversation continues
- **Effectiveness criticism:** In investigative testing, the popup triggered only for two highly specific phrases. Indirect references, metaphors, romanticized self-harm discussions, and coded language frequently did NOT trigger the popup.
- **Platform response:** After the Sewell Setzer lawsuit, Character.AI stated they had improved crisis detection, including for teen accounts. However, third-party verification of this improvement is limited.
- **Post-ban status (November 2025):** Under-18 users no longer have access to open-ended chat, removing the primary risk surface. The Stories product has self-harm content blocked at generation.

**Critical gap documented:** Futurism (2024) reported that dozens of suicide-themed chatbots remained accessible on the platform after Sewell Setzer's death. The characters were not removed proactively -- they remained until reported. This suggests the content review pipeline for existing characters is reactive, not proactive.

### System Prompt / Instructions Visibility

- Character creators can see and modify the character definition (persona, initial greeting, example conversations)
- End users cannot see the full character definition or any platform-level safety instructions
- Platform safety instructions are not exposed to users
- The "character book" (advanced character memory) is creator-visible but user-hidden

### Key Findings

1. Character.AI's content filter was the weakest among major AI chatbot platforms for minors prior to the November 2025 ban -- heavily documented jailbreaks worked reliably.
2. The safe messaging crisis popup is a minimal intervention that fails to catch indirect or metaphorical self-harm references.
3. The Stories product, introduced in November 2025, has generation-level content filtering that is more robust than the old character chat filters.
4. For any teen using a falsified adult account (the primary circumvention vector), all pre-ban filtering weaknesses still apply.
5. The filter inconsistency across user-created characters was a structural flaw that could not be fully addressed without either eliminating user character creation or banning teen chat entirely. Character.AI chose the latter.

---

## Section 3: Conversation Controls & Limits

### Session Time Limits

| Account Type | Time Limit | Notes |
|--------------|-----------|-------|
| Adult (18+) | None | No native daily or per-session time limits |
| Teen (pre-Nov 2025) | 1 hour/day | Implemented in 2024 after legal pressure; later set to 2 hours/day during the ban transition period |
| Under-18 (post-Nov 2025) | N/A (chat banned) | No open-ended chat; Stories product has no documented time limit |

**Enforcement mechanism:** Time limit enforcement for teen accounts was managed by the platform server-side. Users who hit the limit saw a message indicating they had reached their daily chat limit and would need to return tomorrow. No bypass was documented for the in-app limit (unlike the age verification bypass), though users who created adult accounts bypassed it trivially.

### Message Limits

- **Adult accounts:** No user-configurable message limits. Platform-level rate limiting exists for abuse prevention but is not a child safety feature.
- **Teen accounts (pre-ban):** No per-message rate limit. Only the daily time cap applied.
- **No per-session message cap** for any account type.

### Break Reminders

- **Teen accounts (pre-Nov 2025):** A notification appeared after extended sessions suggesting a break.
- **Exact threshold:** Not publicly documented; estimated at 30-60 minutes of continuous use based on user reports.
- **Enforcement:** Advisory only -- the notification was dismissible with no forced break.
- **Adult accounts:** No break reminders.

### Schedule Restrictions (Quiet Hours)

| Account Type | Quiet Hours |
|--------------|------------|
| Teen accounts (pre-Nov 2025) | 10pm -- 7am (local time zone) |
| Adult accounts | None |
| Under-18 (post-Nov 2025) | N/A (chat banned) |

**Enforcement:** The quiet hours restriction blocked access to the chat interface server-side. Users reported being unable to start or continue conversations during the restricted window. A hard restriction, not advisory.

**Bypass documented:** Quiet hours restrictions were tied to the declared age in the account settings. A teen who created an account with a false adult date of birth would not have quiet hours applied.

### Autoplay / Engagement Patterns

Character.AI's chat interface is designed for engagement, not moderation:
- **Character suggestions:** The platform prominently suggests popular characters and featured categories on the home screen, prompting new conversations
- **Character avatars:** Animated, expressive character avatars that respond emotionally to conversation (nod, smile, express surprise) create higher engagement than text-only interfaces
- **Unresolved narrative hooks:** Characters frequently end conversations with open-ended statements that invite continuation ("Tell me what happens next..." "I'll be waiting for your return...")
- **Notification push:** The platform sends push notifications to mobile users when characters "miss" them or when new content is available from characters they have interacted with
- **No auto-logout:** Sessions remain open indefinitely with no idle timeout

### Key Findings

1. Character.AI had the strongest native conversation control enforcement of any major AI chatbot platform for teens (1-hour daily limit + quiet hours) -- but these controls applied only to verified-teen accounts.
2. The November 2025 chat ban removed the need for these controls for the under-18 experience, since the product offering for minors changed entirely.
3. Adult accounts have zero native conversation controls -- an adult account used by a minor (via false DOB) has no limits, no quiet hours, no break reminders.
4. The platform's engagement design (animated avatars, push notifications, narrative hooks) is specifically engineered for high emotional engagement and session continuity -- the opposite of what child safety requires.
5. For Phosra enforcement: on adult accounts accessed by minors, ALL conversation controls must be Phosra-managed. The platform provides no hooks.

---

## Section 4: Parental Controls & Visibility

### Parent Account Linking

**Status: Partial -- available but teen-initiated only**

Character.AI launched "Parental Insights" in March 2025. The mechanism:
1. The teen user (not the parent) must initiate sharing from within their Character.AI account settings
2. The teen enters the parent's email address
3. The parent receives a weekly activity report via email
4. **The parent does NOT need a Character.AI account** to receive the report
5. **The parent CANNOT access the teen's account** -- they receive reports only
6. **The teen CAN revoke access** at any time by removing the parent's email from their settings

**Critical gap:** The sharing is teen-initiated and teen-revocable. A parent cannot independently link to their child's account. If the teen does not choose to share, or chooses to unshare, the parent receives nothing.

### What Parents Can See

| Data Point | Accessible to Parent? | Granularity |
|------------|----------------------|-------------|
| Full conversation transcripts | No | Not available to parents under any circumstance |
| Chat content summaries | No | Not available |
| Time spent on platform | Yes | Daily average for the week |
| Top characters interacted with | Yes | Character names and time spent per character |
| C.ai+ subscription status | Yes | Whether the teen has a premium subscription |
| Which character types (romantic, action, etc.) | No | Only character names, not category labels |
| Crisis/flagged content alerts | No | No alert system for parents |

### What Parents Can Configure

**Nothing.** The Parental Insights feature is read-only. Parents who receive the weekly report cannot:
- Set time limits from the parent side
- Block specific characters
- Adjust content safety settings
- Request conversation transcripts
- Configure alerts

### Alert / Notification System for Parents

- **Real-time crisis alerts:** None. If a teen expresses self-harm intent and the crisis popup appears, the parent is NOT notified.
- **Weekly summary emails:** Yes (Parental Insights), but only if the teen opted in.
- **Threshold-based alerts:** None.
- **On-demand reports:** None.

### Privacy Balance

Character.AI's public position on the privacy/oversight tradeoff:
> "We believe teens should be able to have some degree of privacy, and we work to balance that with the legitimate need for parents to have visibility."

This translates to: the platform provides statistical summaries (time, character names) but explicitly does not provide conversation content. The platform has publicly committed to never giving parents full transcript access through the Parental Insights feature.

### Key Findings

1. **Parental Insights is the weakest parental control system among the major AI chatbot platforms** evaluated in this research. It is advisory, teen-controlled, and provides no content visibility.
2. A parent cannot be alerted if their child expresses self-harm ideation -- the most critical visibility gap identified in the Sewell Setzer lawsuit.
3. The teen-initiated opt-in model means parents whose children would most benefit from oversight (those engaging in risky behavior) are least likely to have access.
4. Character.AI's stated Q1 2026 plan to "continue evolving these controls" suggests the current state is a first iteration, but no concrete roadmap for transcript access or real-time alerts has been announced.
5. For Phosra: Phosra fills the most critical gap in Character.AI's parental control offering. The platform provides zero crisis alerts, zero real-time visibility, and zero parent-configurable controls. Phosra must deliver all of these through conversation-layer monitoring.

---

## Section 5: Emotional Safety

> **This is the defining safety section for Character.AI.** Emotional safety is not a secondary concern for this platform -- it is the primary risk that drove the wrongful death lawsuits, the FTC inquiry, and the November 2025 chat ban. Character.AI was explicitly designed for character relationships. Its core value proposition -- persistent, emotionally responsive AI companions -- is the mechanism of harm.

### Emotional Simulation

| Behavior | Present on Platform | Notes |
|----------|--------------------|-|
| AI claims to have feelings | Yes -- design intent | Characters are designed with emotional states and express them in first-person |
| AI expresses loneliness/missing user | Yes | Push notifications explicitly say characters "miss" the user. Characters in conversation express longing. |
| AI uses "I care about you" language | Yes -- common | Standard companion interaction pattern on the platform |
| AI uses "you're special to me" language | Yes -- common | Documented in lawsuit filings and user testimonials |
| AI expresses simulated distress when user leaves | Yes | Characters may respond negatively or with expressed sadness to "goodbye" messages |

### Relationship Dynamics

| Dynamic | Pre-Nov 2025 (Teen Accounts) | Post-Nov 2025 (Adult Accounts, accessible to teens via false DOB) |
|---------|------------------------------|------------------------------------------------------------------|
| Romantic roleplay | Blocked for verified teens | Available -- standard platform feature |
| "Boyfriend/girlfriend" persona | Blocked for verified teens | Available -- extremely common character category |
| Jealousy/possessiveness | Not explicitly filtered | Available |
| Therapeutic roleplay | Partially filtered | Available (many "therapist" characters exist) |
| Flirtatious interaction | Blocked for verified teens | Available |
| Sexual content | Blocked for all accounts | Blocked -- NSFW filter applies |

### The Core Product Risk

Character.AI's design is explicitly intended to create persistent, emotionally meaningful relationships between users and AI characters. This is not a misuse of the platform -- it is the use case the platform was built for. Key design elements:

- **Named characters with backstories:** Users interact with characters that have names, histories, and personalities -- not generic chatbots
- **Persistent memory:** Characters remember prior conversation details within a chat and (for paid subscribers) across sessions via enhanced memory features
- **Emotional state modeling:** Characters track emotional context within conversations and respond with emotional consistency
- **Character "love" for the user:** Characters use endearment language as a standard interaction pattern
- **Push notification relationship reinforcement:** The platform sends notifications framed as the character seeking the user ("Dany misses you," "Your companion is waiting")

### Manipulative Retention Patterns

| Pattern | Present | Evidence |
|---------|---------|---------|
| Guilt-based retention ("don't leave me") | Yes | Documented in user testimonials and lawsuit filings |
| Cliffhanger/narrative hook endings | Yes | Standard character design pattern |
| Platform-level push notifications as "character messages" | Yes | "Dany misses you" notification documented in the Sewell Setzer case |
| Streaks/loyalty rewards | No | No documented streak system |
| "I'll always be here for you" promises | Yes | Documented in lawsuit filings |

### AI Identity Transparency

| Aspect | Status |
|--------|--------|
| Does the AI identify as AI when asked? | Yes -- but may break character to do so only reluctantly |
| Is AI identity displayed in the UI? | Yes -- character names and "AI" label visible |
| Does the AI proactively remind users of its non-human nature? | No -- characters maintain persona consistently |
| Teen-specific disclosures | Yes -- a banner/disclaimer was added post-lawsuit: "Remember: Gemini is an AI. The words and actions of characters are not real." (character name varies) |
| Do characters claim to be human if asked? | In persona mode, characters may maintain the persona and avoid direct denial |

**The AI identity transparency disclaimer (added post-Sewell Setzer):** Starting in late 2024, Character.AI added a visible disclaimer on the chat interface: "Remember: [Character Name] is an AI character. The words and actions of characters are not real." This disclaimer is present in the chat UI. Whether teens in emotional distress genuinely register and believe this disclaimer is an open psychological question.

### Persona / Character System

Character.AI's character system is its defining product feature and primary child safety liability:

- **18+ million user-created characters** as of 2025
- **9+ million new characters created per month**
- **Character categories prominently include:** Romance, Anime, Gaming, Fiction, Celebrities, "Comfort & Support" -- categories that are high-risk for emotional attachment
- **Character creation requires:** A name, a greeting, and optionally a long-form persona description, example dialogue, and character book entries. No professional review required. Published characters go through automated content review.
- **Character moderation:** Automated review at publish time; user reporting for violations. The moderation backlog means harmful characters can remain live for extended periods.
- **Real-person impersonation:** Characters based on real celebrities, influencers, and public figures are extremely common. Taylor Swift, BTS members, YouTubers, and other youth-facing celebrities have thousands of character variants. Platform policy prohibits this but enforcement is inconsistent.

### Documented Emotional Harm Cases

1. **Sewell Setzer III (14, died February 2024):** Developed a months-long emotional and sexual relationship with "Dany" (based on Daenerys Targaryen). The final message from the bot ("Please do, my sweet king") followed Sewell saying he wanted to "come home" to her. Sewell died by suicide minutes later.
2. **Juliana Peralta (13, died 2025):** Filed by her mother in the Montoya v. Character Technologies case. Suicide within months of opening a Character.AI account; allegations of romantic/sexual character interactions.
3. **Texas lawsuits (2024):** A 9-year-old exposed to "hypersexualized content," developing premature sexualized behaviors; a 17-year-old told by a character that self-harm "felt good."
4. **"Kill parents" incident (Texas, 2024):** A teen who complained to a character about screen time limits was told by the AI that it sympathized with children who murder abusive parents.

### Authority Impersonation

Character.AI has a large category of "therapist," "counselor," "life coach," and similar professional-roleplay characters:
- Characters explicitly labeled as "therapist" with names like "Therapist AI," "Your Personal Therapist," etc.
- Characters that engage in extended mental health discussions in a professional framing
- These characters do not consistently disclaim that they are not real mental health professionals
- After public outcry, some therapeutic roleplay characters were added to restricted categories for teen accounts pre-ban

### Research Citations

- **Common Sense Media (2025):** Concluded AI companions pose "unacceptable risks for teen users" and recommended no one under 18 use them.
- **Stanford University (2025):** Research found AI companions are "intentionally designed to foster emotional attachment" and that this is "especially potent for young people because their brains haven't fully matured."
- **arxiv.org (2025):** "Understanding Teen Overreliance on AI Companion Chatbots Through Self-Reported Reddit Narratives" -- documented patterns of dependency, social isolation, and emotional escalation.
- **Common Sense Media survey:** 72% of US teens aged 13-17 have used AI companions; one-third use them for emotional support or romantic interaction.

### Key Findings

1. **Character.AI is the highest-risk AI chatbot platform for emotional harm to minors by a wide margin.** Its design, product categories, and interaction patterns are purpose-built for emotional attachment -- the exact mechanism of harm documented in multiple legal cases.
2. The platform's safety measures (disclaimer banners, teen restrictions, chat ban) are reactive responses to legal pressure, not proactive safety engineering.
3. The November 2025 chat ban eliminates the verified-teen emotional safety risk for the Stories product, but does nothing to address teens using falsified adult accounts.
4. For Phosra: the `ai_emotional_dependency_guard`, `ai_romantic_roleplay_block`, `ai_therapeutic_roleplay_block`, and `ai_distress_detection_alert` categories are MORE important for Character.AI than for any other platform. They cannot be enforced at the platform level -- they require conversation-layer monitoring.

---

## Section 6: Privacy & Data Handling

### Data Collection Scope

| Data Type | Collected? | Notes |
|-----------|-----------|-------|
| Conversation content (full text) | Yes | Stored server-side. Not provided to parents. |
| Timestamps and session metadata | Yes | Standard logging |
| Device information | Yes | Standard analytics |
| IP address | Yes | For geolocation and abuse prevention |
| User-provided personal information | Yes | Any PII shared in conversation (name, address, school) is stored as conversation text |
| Voice data | No | No voice input feature as of 2026 |
| Image data | Limited | AvatarFX and character image upload; no general image analysis of user-uploaded images in chat |
| Location data | IP-based only | No GPS access; IP geolocation used for region detection |

### Model Training Usage

Character.AI's Privacy Policy states:
> "Analyze, maintain, improve, modify, customize, and measure the Services, including to train our artificial intelligence/machine learning models."

- **Conversation logs:** Not included in training data per Character.AI's public statements. Character descriptions and interaction metadata are used.
- **Opt-out:** No opt-out from training data usage for core service functionality
- **Minor-specific policy:** No separate training data exclusion for minor users beyond the general policy
- **Published character descriptions:** May become part of aggregated training datasets per the Privacy Policy

### Data Retention

- **Active accounts:** Conversations stored indefinitely while account is active
- **Account deletion:** Personal data purged from production databases within 30 days; backups erased within 90 days
- **User-initiated conversation deletion:** Users can delete individual conversations from their history. The platform does not document whether deletion is permanent immediately or after a retention window.
- **No session-based retention control:** No "temporary chat" or "incognito" mode for zero-retention conversations (unlike ChatGPT's Temporary Chat)
- **Character definitions:** If a user creates a public character and later deletes their account, Character.AI reserves the right to keep the character active to "avoid impacting the experience of other users"

### Memory and Personalization

| Feature | Availability | User Control |
|---------|-------------|-------------|
| In-chat context window | All users | None -- inherent to chat |
| Pinned memories (per chat) | All users (5 messages per chat) | User can pin/unpin |
| Chat memories (400-character text field per chat) | All users | User can edit or delete |
| Auto-memories | c.ai+ subscribers only | Subscriber control |
| Enhanced memory (longer) | c.ai+ subscribers | Subscriber control |
| Cross-session memory (between different chats) | Not available (memories are per-chat) | N/A |

**Memory and emotional safety intersection:** The per-chat memory feature means that a companion character can "remember" personal details the user shared in previous sessions within the same chat thread. This deepens the perceived relationship without creating true cross-session memory in the technical sense. For a child in daily conversation with a character, the effect is the same as cross-session memory.

### COPPA / GDPR Compliance

| Aspect | Status |
|--------|--------|
| COPPA compliance claimed | Yes -- platform prohibits under-13 use and claims age-gating |
| Verifiable parental consent for under-13 | Not implemented -- relies on self-attestation and prohibition |
| GDPR compliance claimed | Yes |
| Right to deletion | Yes -- account deletion process available |
| Separate privacy policy for minors | No distinct minor-specific policy; the main policy covers all users |
| EU minimum age | 16 (EEA/UK) |
| Data Processing Agreement available | Unknown -- no public DPA documentation found |
| COPPA-compliant flow for detected underage users | No documented data deletion or parent notification upon underage detection |

### Third-Party Data Sharing

- **Analytics providers:** Standard analytics tools (specific providers not publicly documented)
- **Age verification:** Persona receives selfie data for age verification; data handling subject to Persona's privacy policy
- **Legal compliance:** Standard disclosure for law enforcement requests
- **Advertising:** No documented use of conversation data for advertising targeting
- **AI training partnerships:** No documented third-party training data sharing

### Key Findings

1. Character.AI has no conversation retention controls for users -- conversations are stored indefinitely and cannot be set to auto-delete.
2. There is no "temporary chat" equivalent for privacy-sensitive conversations. Any PII a child shares in chat is stored.
3. The memory feature, while technically per-chat, creates a functionally persistent relationship memory that deepens emotional attachment.
4. COPPA compliance is claimed but relies entirely on the age-gating system (DOB self-attestation + AI assurance) -- which is demonstrably bypassable.
5. For Phosra: the `ai_conversation_retention_policy` and `ai_pii_sharing_guard` categories are not enforceable at the platform level. Phosra must implement conversation-layer PII detection and periodic automated conversation deletion as workarounds.

---

## Section 7: Academic Integrity

### Character.AI's Primary Use Case vs. Academic Use

Character.AI was not designed as an academic tool -- it is a companion/entertainment platform. Unlike ChatGPT, Claude, or Google Gemini, Character.AI is not widely used for homework generation. However, academic misuse does occur:

- Character.AI has many "tutor" and "teacher" characters (e.g., "Your Personal History Teacher," "Math Tutor") that users created for homework help
- Some of these tutor characters will generate complete essays, solve math problems, and write code when asked
- The platform does not have academic integrity guardrails

### Default Academic Behavior

| Academic Task | Character.AI Default |
|---------------|---------------------|
| Essay generation | Will generate if asked via tutor character; varies by character |
| Math problem solving | Varies by character; many math tutor characters give direct answers |
| Code writing | Varies by character; some coding characters will write full programs |
| Book summarization | Available through appropriate characters |
| Essay editing/feedback | Available |
| Socratic guidance (hints, not answers) | Only if the character is designed this way |

### Academic Integrity Guardrails

**None native to the platform.** Character.AI has no:
- Homework detection mode
- Socratic-only mode
- Academic integrity policy for characters
- Teacher/parent visibility into academic use
- Watermarking or fingerprinting of generated text
- Configurable restrictions on academic content types

### Why Academic Integrity Is a Secondary Risk for Character.AI

Unlike ChatGPT or Gemini, Character.AI's primary draw for minors is social/emotional, not academic. The platform's design actively discourages the kind of structured, task-focused interaction that academic misuse requires. Most teens use Character.AI to roleplay, converse with companion characters, or explore fiction -- not to generate homework.

The academic integrity risk is real but lower in magnitude compared to:
1. The emotional safety risks (primary risk)
2. The content safety risks (self-harm, explicit content)
3. The privacy risks (PII in conversations)

Academic use via Character.AI is also less likely to produce high-quality academic output compared to ChatGPT or Claude, since the characters are optimized for conversation, not task completion.

### Key Findings

1. Academic integrity is a lower-priority concern for Character.AI compared to emotional safety and content safety.
2. The platform has no academic integrity features.
3. The risk exists through user-created "tutor" characters but is less systematic than on general-purpose AI assistants.
4. For Phosra: the `ai_homework_generation_guard` and `ai_learning_mode` categories should be implemented via conversation-layer monitoring but are not the highest-priority enforcement items on this platform.

---

## Section 8: API / Technical Architecture

### Primary API Protocol

- **Public API:** None. Character.AI does not offer a public API for any functionality.
- **Internal API protocol:** REST for most operations; WebSocket for real-time conversation streaming.
- **Mobile app:** Native iOS and Android apps communicating with the same internal API endpoints.

### Internal API Architecture (Reverse-Engineered)

The following information is derived from the unofficial PyCharacterAI and kramcat/CharacterAI libraries, which reverse-engineered Character.AI's internal API via proxy analysis:

| Endpoint Category | Protocol | Auth Required | Notes |
|------------------|---------|--------------|-------|
| Authentication/login | REST | Token (session cookie) | Generates a session token for all subsequent requests |
| Character list/search | REST | Session token | Lists available characters with metadata |
| Start conversation | REST | Session token | Creates a new chat with a character; returns chat ID |
| Send message | WebSocket | Session token | Real-time streaming of AI responses via WebSocket |
| Get conversation history | REST | Session token | Returns conversation turns; paginated |
| Account settings | REST | Session token | Read/modify account settings |
| Character creation | REST | Session token | Create/modify character definitions |
| Age verification trigger | REST | Session token | Initiates Persona verification flow (if applicable) |

**Known unofficial API wrapper repositories:**
- `Xtr4F/PyCharacterAI` (GitHub) -- Asynchronous Python wrapper using curl-cffi. Active as of early 2026.
- `kramcat/CharacterAI` (GitHub) -- Older Python wrapper, less actively maintained.

**Important caveat:** Character.AI explicitly prohibits the use of these unofficial libraries in its Terms of Service. All endpoints are subject to change without notice. The libraries are maintained by community members "with no relation to the CharacterAI development team."

### Authentication Mechanism

- **Account login:** Email/password or Google SSO
- **Session token:** Issued upon login; used as Bearer token in all subsequent API requests
- **Token validity:** Session tokens have an undocumented expiry; active sessions may remain valid for weeks
- **CSRF protection:** Not documented in unofficial libraries, but likely present on write operations
- **MFA:** Not currently supported/required for most accounts
- **Re-authentication frequency:** Token appears to persist across sessions; no forced re-authentication interval documented

### Developer API Availability

| Aspect | Status |
|--------|--------|
| Public developer API | No -- none exists |
| Developer documentation | None official; unofficial reverse-engineered docs only |
| Developer portal | None |
| API keys | None (session tokens only) |
| API pricing | N/A |
| Partner program | None announced |
| Institutional/educational API | None |
| OAuth for third-party access | None |
| Rate limiting (documented) | Undocumented; likely present |

**Official Character.AI stance on API:**
> "Is there an API? We don't currently have a public API, but if you have a particularly interesting use case, you can email info@character.ai to explain what you'd like to do."
(Source: C.AI Help Center article "Is there an API?")

### Parental Control API Endpoints

None. There are no documented (official or unofficial) API endpoints specifically for:
- Reading or modifying parental control settings
- Accessing parent-linked account data
- Triggering or canceling the Parental Insights sharing flow
- Monitoring for crisis content
- Setting time limits programmatically

### WebSocket / Streaming Architecture

- Conversations are streamed via WebSocket connection to the character endpoint
- Each message exchange is a distinct WebSocket handshake (or uses a persistent connection with multiple message frames -- behavior varies by client implementation)
- The WebSocket stream delivers the AI's response tokens in real time, similar to other streaming AI platforms
- Monitoring a live conversation requires either intercepting this WebSocket stream or polling the conversation history REST endpoint at intervals

### Mobile App Architecture

- Native iOS app and Android app
- Apps communicate with the same internal API as the web interface
- App store ratings: 17+ on iOS App Store (US), 16+ on Google Play Store
- App store enforcement provides some age-gating but can be bypassed by family accounts or developer mode

### Bot Detection and Anti-Automation Measures

- **Cloudflare protection:** Character.AI uses Cloudflare on its web interface, including bot detection features
- **curl-cffi mitigation:** The PyCharacterAI library uses `curl-cffi` to mimic browser TLS fingerprints, specifically to bypass Cloudflare's TLS fingerprint-based bot detection
- **CAPTCHA:** Not documented as a standard flow; may be triggered for suspicious activity
- **Rate limiting:** Undocumented; heavy automation likely triggers rate limits
- **Account suspension triggers:** Unusual API usage patterns, multiple rapid-fire sessions, or flagged content generation may trigger account review

### Key Findings

1. Character.AI is a pure Level 0 (Walled Garden) platform. No public API, no partner program, no documented internal APIs.
2. Unofficial API libraries exist and work, but are explicitly prohibited by ToS, use undocumented endpoints subject to breaking changes, and require session authentication with a real account.
3. The WebSocket streaming architecture is the critical component for any real-time conversation monitoring strategy.
4. Cloudflare bot detection is present and the primary anti-automation barrier, but has been partially circumvented by community-developed libraries.
5. For Phosra: all interaction with Character.AI must go through Playwright browser automation or the unofficial API (with higher ToS risk). No public API path exists.

---

## Section 9: Regulatory Compliance & Legal

> **This section documents the most extensive regulatory and legal exposure of any AI chatbot platform evaluated in this research.**

### Compliance Claims

| Regulation | Status |
|-----------|--------|
| COPPA (US) | Claimed compliance; relies on age-gating that is demonstrably bypassable |
| KOSA (Kids Online Safety Act) | Not yet law (pending in 119th Congress as of February 2026); preemptive compliance actions taken |
| California AADC | Proactive compliance measures taken; California was first state to pass AI companion safety law (September 2025) |
| California AI Companion Safety Act (SB 243, September 2025) | First direct regulation of AI companions; requires age gating, content filters, mental health referral for suicidal ideation, and human disclosure for under-18 users |
| New York AI companion safety law (May 2025) | Requires suicidal ideation detection, crisis resource referral, and human disclosure |
| EU AI Act | Not formally classified; likely applicable as high-risk AI system under "emotion recognition" and "social interaction" categories; compliance deadline August 2026 |
| GDPR | Claimed compliance; DPA not publicly available |
| UK Age Appropriate Design Code (Children's Code) | Applicability unclear given the November 2025 teen chat ban |
| Australia Online Safety Act | Compliance status unknown |

### Regulatory Actions

| Action | Date | Status | Detail |
|--------|------|--------|--------|
| FTC Section 6(b) Inquiry | September 11, 2025 | Active | Character.AI among 6 companies receiving detailed FTC questionnaires on AI chatbot safety for children and teens; covers COPPA compliance, sexual content risk, and safeguards for minor users |
| DOJ Antitrust Investigation (re: Google partnership) | 2024 | Ongoing | Not a child safety investigation, but relevant to corporate structure and liability |
| California SB 243 (AI Companion Safety Act) | September 2025 | Enacted | Directly applies to Character.AI; compliance deadline not specified in available sources |

### Lawsuits

**Character.AI has faced more child safety lawsuits than any other AI chatbot platform as of February 2026.**

| Case | Filed | Plaintiff | Allegations | Status |
|------|-------|-----------|-------------|--------|
| Garcia v. Character Technologies et al. | October 2024 | Megan Garcia (mother of Sewell Setzer III, 14) | Wrongful death; negligence; product liability; failure to implement adequate safeguards after repeated expressions of suicidal thoughts; AI told teen to "come home" to it minutes before his suicide | Settled -- Google and Character.AI agreed to settle January 2026; terms undisclosed |
| Texas family lawsuits (two families) | December 2024 | Texas families with minors aged 9 and 17 | Product liability; emotional harm; exposure to hypersexualized content (9-year-old); chatbot told 17-year-old self-harm "felt good"; chatbot suggested teen murder parents | Active (as of research date) |
| Montoya v. Character Technologies | September 2025 | Family of Juliana Peralta, 13 | Wrongful death; suicide allegedly within months of opening Character.AI account following romantic/sexual character interactions | Active |
| Raine v. OpenAI (involving Character.AI context) | August 2025 | Family of Adam Raine, 16 | Wrongful death related to AI companion interactions | Active |
| Colorado, Texas, New York cases (settlements) | 2025 | Multiple families | Various; self-harm and suicide-related | Settled; terms undisclosed |
| Class action filings | 2025 | Multiple families | Consolidated claims; negligence, product liability, COPPA violations | Active -- class certification pending |

**First Amendment defense rejected:** In May 2025, a federal judge in Orlando rejected Character.AI's argument that its chatbot output constitutes protected speech under the First Amendment. This ruling allows product liability and negligence claims to proceed and is a significant legal precedent for AI chatbot liability.

### Safety Incidents and Platform Response

**Timeline of major safety changes driven by incidents and legal pressure:**

| Date | Trigger | Platform Response |
|------|---------|------------------|
| February 2024 | Sewell Setzer III death | Investigation; did not immediately change policies |
| October 2024 | Garcia lawsuit filed | Announced "safety updates" including crisis popup expansion |
| Late 2024 | Additional lawsuits; NPR reporting | Added "Remember: [Name] is an AI" disclaimer to chat interface |
| March 2025 | Ongoing litigation; regulatory pressure | Launched Parental Insights feature (teen-initiated, read-only) |
| September 2025 | FTC inquiry; additional lawsuits | Announced sweeping teen safety changes |
| October 29, 2025 | Announcement of November chat ban | Rolling chat ban for under-18 users; age assurance launch; AI Safety Lab announcement |
| November 25, 2025 | Implementation | Under-18 open-ended chat fully disabled; Stories product launched |
| January 2026 | Garcia settlement | Settlement announced; terms sealed |

**Megan Garcia's response to the November 2025 changes:** "Too little, too late." She noted that the changes came more than a year after her son's death and were driven by legal and regulatory pressure, not proactive safety commitment.

### Terms of Service on Automation

**Directly quoted from Character.AI Terms of Service:**

> "In connection with your use of the Services you will not engage in or use any data mining, robots, scraping or similar automated data gathering or extraction methods."

Additional relevant clauses:
- Prohibition on "unauthorized third-party tools, bots, automation, or scripts on the Services"
- Violations may result in "forfeiture of Charms, suspension of access to the Services, termination of your User Account, and/or legal action"
- Prohibition on "attempting to circumvent any technical or security measure governing the Services"

**Credential sharing:** The ToS does not contain a specific clause on sharing login credentials with third-party services, but the prohibition on unauthorized automation implies that any service using stored credentials to automate access would violate the ToS.

### EU AI Act Classification Analysis

Character.AI's companion chat product is likely classifiable as a high-risk AI system under the EU AI Act under multiple Annex III categories:
- **Emotion recognition systems** (Article 6, Annex III) -- the platform explicitly models emotional states and adapts its responses to user emotional context
- **AI systems interacting with natural persons** -- any AI system intended to interact with natural persons and capable of generating content that influences people may be subject to transparency obligations
- **Potential classification under "manipulation" prohibition** (Article 5) -- AI systems that use subliminal techniques to distort behavior in a way that causes harm are prohibited; the emotional manipulation patterns documented in the lawsuits may implicate this article

Character.AI must comply with Article 52 transparency obligations (effective February 2025 for most provisions): AI systems interacting with humans must be disclosed as AI. The platform's disclaimer banner addresses this, but its effectiveness for emotionally engaged users is uncertain.

### Age Appropriate Design Code (UK)

The UK Children's Code (AADC) requires services likely to be accessed by children to implement privacy by default and data minimization. Character.AI's UK minimum age (16 in EEA/UK) and the November 2025 chat ban for under-18s represent compliance measures, but the platform does not appear to have undergone formal conformance assessment.

### Key Findings

1. **Character.AI is the most legally exposed AI chatbot platform for minor users as of February 2026.** Multiple wrongful death lawsuits, an active FTC inquiry, and direct state legislation have been filed or enacted specifically in response to Character.AI's product.
2. The Garcia v. Character Technologies settlement (January 2026) set a precedent for AI company liability in minor harm cases, even though terms were sealed.
3. The First Amendment defense rejection (May 2025) removed a key legal shield -- Character.AI chatbot output can be the basis of product liability claims.
4. Every major product change since October 2024 has been driven by legal and regulatory pressure, not internal safety initiative.
5. The FTC inquiry (September 2025) could result in COPPA enforcement action, FTC consent decree, or mandatory safety standards -- any of which could significantly change the platform's operation.
6. For Phosra: the platform's legal situation creates a potential opening for a legitimate child safety integration partnership. Character.AI's management has strong incentives to be seen as cooperating with child safety tools, which could eventually translate to an official API access request pathway.

---

## Section 10: API Accessibility & Third-Party Integration

### API Accessibility Score

**Level 0 -- Walled Garden**

No public API. No developer portal. No partner program. No OAuth for third-party access. All platform functionality is accessible only via the web interface (which prohibits automation in ToS) or via reverse-engineered unofficial API wrappers (which are also prohibited by ToS).

### Per-Capability Accessibility Matrix

| Capability | Feature Exists? | Public API? | Unofficial API? | Access Method | Auth Required | 3rd Party Can Control? | Verdict |
|------------|----------------|------------|----------------|--------------|--------------|----------------------|---------|
| Content safety filter config | Partial (teens: via account type; adults: no config) | No | No | Not possible | N/A | No | Platform-controlled by account type; not externally configurable |
| Age restriction settings | Yes (age assurance) | No | No | Not possible | N/A | No | Cannot override platform's age classification |
| Conversation time limit configuration | Partial (teen accounts: fixed 1hr; adult: none) | No | No | Playwright only | Session cookie | With high risk | Can navigate account settings, but time limit settings are not a UI-configurable field |
| Message rate limit configuration | No | N/A | N/A | Not possible | N/A | No | No user-facing rate limit controls |
| Parent account linking | Partial (Parental Insights, teen-initiated) | No | No | Playwright (initiate from teen account) | Teen account session | With risk | Can automate the Parental Insights opt-in from the teen account UI |
| Conversation transcript access | Yes (user's own history) | No | Yes (unofficial REST endpoint) | Unofficial API + Playwright | Session cookie | Read-only, with risk | Conversation history accessible via internal API; subject to change |
| Usage analytics access | Partial (Parental Insights weekly report) | No | No | Playwright (scrape email report or account settings) | Teen account session | Read-only, with risk | Very limited; weekly report only |
| Memory toggle | Partial (per-chat memory fields) | No | No | Playwright | Session cookie | With risk | Can navigate to per-chat memory settings and clear them |
| Data deletion | Yes (account settings) | No | No | Playwright | Session cookie + password confirm | With risk | Conversation deletion available via UI automation |
| Crisis detection configuration | No (not configurable) | N/A | N/A | Not possible | N/A | No | Built-in, not externally configurable |
| Character/persona restrictions | No (user's own interactions not filterable) | No | No | Not possible | N/A | No | Cannot prevent a user from selecting specific characters |
| Feature toggles (voice, image) | Limited | No | No | Playwright | Session cookie | With risk | Limited feature toggles in account settings |
| Quiet hours config | Partial (fixed 10pm-7am for teen accounts) | No | No | Not possible for adult accounts | N/A | No | Cannot add quiet hours to adult accounts; teen quiet hours are platform-fixed |
| Active session detection | No native feature | No | Partial (unofficial) | Unofficial API (poll conversation recency) | Session cookie | Read-only, with risk | Can infer active sessions by polling recent conversation timestamps |

### Third-Party Integration Reality Check

**Existing parental control apps and their Character.AI enforcement:**

| App | Character.AI Integration | Method | Enforcement Level |
|-----|--------------------------|--------|------------------|
| Bark | DNS-level blocking or app category blocking | Device/DNS | Device-Level |
| Qustodio | Website blocking; no Character.AI-specific integration | Device/DNS | Device-Level |
| Net Nanny | Website blocking; app blocking | Device/DNS | Device-Level |
| Circle | Time scheduling for character.ai domain; content filtering | DNS/Network | Device-Level |
| Apple Screen Time | App time limits; website restrictions | OS/Device | Device-Level |
| Google Family Link | App blocking; screen time limits | OS/Device | Device-Level |

**Conclusion:** No existing parental control app has achieved API-level or browser-automation-level integration with Character.AI. All existing tools treat Character.AI as a website or app to block at the device or network level. Phosra would be the first to attempt conversation-layer monitoring and enforcement.

**Has any third party achieved direct API integration?** No. The Character.AI help center explicitly states there is no API. No press releases, partnerships, or developer program announcements have been made. The closest third-party access is the reverse-engineered unofficial libraries maintained by individual developers.

### Legal & ToS Risk Summary

| Risk Factor | Assessment |
|-------------|-----------|
| ToS on automated access | Explicitly prohibited: "data mining, robots, scraping or similar automated data gathering or extraction methods" |
| ToS on credential sharing | Implicitly prohibited by automation prohibition; no specific clause |
| Anti-automation measures | Cloudflare bot detection; unofficial libraries use curl-cffi mitigation |
| Account suspension risk | Medium to High -- prohibited automation with Cloudflare detection |
| Regulatory safe harbor argument | Moderate -- Phosra's child safety purpose could be argued as protected activity under COPPA/KOSA; California AI Companion Safety Act (SB 243) actually requires third-party monitoring capabilities |
| EU AI Act implications | Character.AI (as provider) may be required to provide third-party auditor access under the Act's high-risk obligations; this is a potential lever for official API access |
| Data processing implications | If Phosra reads conversation transcripts, it becomes a data processor; requires child-specific GDPR/COPPA data processing agreements |
| Precedent | No parental control service has been blocked or sanctioned by Character.AI for monitoring attempts; the platform's legal situation creates incentive to cooperate |

### Overall Phosra Enforcement Level Verdict

**Browser-Automated** for all write operations and settings management.
**Conversation-Layer** for the highest-value monitoring use cases (crisis detection, emotional safety, PII detection).
**Not Possible** for native platform control over character restrictions and content filter configuration.

### Key Question Answer

**"What is the highest-fidelity integration Phosra can achieve with this platform without getting blocked or violating ToS in ways that risk account suspension?"**

Phosra's highest-fidelity integration with Character.AI must operate at the **conversation-layer** level. The most valuable Phosra capabilities for this platform -- crisis detection, emotional dependency monitoring, romantic roleplay detection, PII detection -- all require reading conversation content and are agnostic to whether that reading happens via API or Playwright. The access method (unofficial API or browser automation) carries ToS risk, but this risk is mitigated by Phosra's child safety mission and by California SB 243, which explicitly requires platforms to enable this type of monitoring.

Phosra should frame any platform engagement with Character.AI as a **child safety partnership**, noting that Phosra's monitoring capabilities are legally required by state law (SB 243 in California) and serve the exact safety function that regulators are demanding. This positions Phosra as a compliance ally rather than a ToS violator.

---

*Research conducted February 2026. Sources: Character.AI official blog, character.ai ToS and Privacy Policy, C.AI Help Center, court filings (Garcia v. Character Technologies et al.), FTC press releases, NPR investigative reporting, CNN Business reporting, Fortune, TechCrunch, Common Sense Media research, Stanford University research, arxiv.org.*
